############### Functions ProjectionsBiomed new #########################################################

############### Libraries ###############

# Package loading function, avoiding installation during the function execution
# Constants for required package names
REQUIRED_PACKAGES <- c(
  "pbmcapply", "deldir", "ggplot2", "ggrepel", "ggplotify",
  "grid", "cowplot", "cluster", "FactoMineR", "ABCanalysis",
  "fastICA", "MASS", "Rtsne", "RDRToolbox", "mixOmics",
  "umap", "caret", "pracma", "combinat", "NbClust", "parallel",
  "dplyr", "ape", "combinat", "NMF", "RANN", "pls", "Matrix"
)

############### Main function ###############

# Function to load required R packages
load_required_packages <- function( packages ) {
  missing_packages <- packages[!packages %in% installed.packages( )[, "Package"]]
  if ( length( missing_packages ) ) stop( "Missing packages: ", paste( missing_packages, collapse = ", " ) )
  invisible( lapply( packages, library, character.only = TRUE ) )
}

# Enhanced version of the perform_analysis function

# Function to perform analysis
perform_analysis <- function(datasets,
                             projection_methods = "PCA",
                             clustering_methods = "none",
                             cluster_number_methods = "orig",
                             selected_cluster_metrics = c("cluster_accuracy", "Silhouette_index", "Dunn_index",
                                                          "Rand_index", "DaviesBouldin_index", "dbcv_index",
                                                          "CalinskiHarabasz_index", "inertia", "adjusted_mutual_information"),
                             distance_metric = "euclidean",
                             highlight_best_clustering = FALSE,
                             method_for_hcpc = "ward",
                             label_points = TRUE,
                             highlight_misclassified = FALSE,
                             points_colored_for = "Target",
                             cells_colored_for = "Cluster",
                             palette_target = NULL,
                             palette_cluster = NULL,
                             seed = 42,
                             jitter_one_dimensional_projections = TRUE,
                             nProc = 12) {
  # Validate input parameters
  if (missing(datasets) || length(datasets) == 0) {
    stop("The `datasets` parameter must be a non-empty list of dataset names.")
  }

  if (!distance_metric %in% c("euclidean", "manhattan", "cosine")) {
    stop("Unsupported distance metric. Use one of 'euclidean', 'manhattan', or 'cosine'.")
  }

  if (!is.logical(highlight_best_clustering)) {
    stop("The argument `highlight_best_clustering` must be a logical (TRUE or FALSE).")
  }

  # Ensure random seed is set for reproducibility
  set.seed(seed)

  # Initialize output variables
  projections_plots <- list()          # Store projection-related plots
  all_projection_results <- list()     # Store projection results
  all_clustering_results <- list()     # Store clustering results
  cluster_quality_results <- list()    # Store cluster quality scores

  # Loop through all datasets
  for (dataset_name in datasets) {
    message("Processing dataset: ", dataset_name)

    # Fetch dataset
    dataset <- retrieve_dataset(dataset_name)
    if (is.null(dataset)) {
      warning("Dataset '", dataset_name, "' could not be retrieved. Skipping...")
      next
    }

    # Handle 'Target' with only one level and adjust projection methods
    local_projection_methods <- projection_methods
    if (length(unique(dataset$Target)) == 1 && "PLSDA" %in% local_projection_methods) {
      message("'Target' has only one level. Removing PLSDA from projection methods list.")
      local_projection_methods <- setdiff(local_projection_methods, "PLSDA")
      if (length(local_projection_methods) < 1) {
        warning("No available projection methods for dataset: ", dataset_name, ". Skipping...")
        next
      }
    }

    # Generate projections
    message("Generating projections...")
    tryCatch({
      projection_results <- generate_projections(
        data = dataset,
        projection_methods = local_projection_methods,
        seed = seed,
        jitter_one_dimensional_projections = jitter_one_dimensional_projections,
        nProc = nProc
      )
      all_projection_results[[dataset_name]] <- projection_results
    }, error = function(e) {
      warning("Projection generation failed for dataset '", dataset_name, "' with error: ", e$message)
      next
    })

    # Apply clustering
    message("Applying clustering...")
    tryCatch({
      clustering_results <- apply_clustering(
        projection_results = projection_results,
        clustering_methods = clustering_methods,
        cluster_number_methods = cluster_number_methods,
        method_for_hcpc = method_for_hcpc,
        distance_metric = distance_metric,
        seed = seed,
        nProc = nProc
      )
      all_clustering_results[[dataset_name]] <- clustering_results
    }, error = function(e) {
      warning("Clustering failed for dataset '", dataset_name, "' with error: ", e$message)
      next
    })

    # Calculate cluster quality indices
    message("Calculating cluster stability or quality indices...")
    tryCatch({
      cluster_indices <- calculate_cluster_scores_df(
        clustering_results = clustering_results,
        projection_methods = local_projection_methods,
        clustering_methods = clustering_methods,
        cluster_number_methods = cluster_number_methods,
        distance_metric = distance_metric, 
        nProc = nProc
      )
      cluster_quality_results[[dataset_name]] <- process_cluster_scores(
        cluster_scores_df = cluster_indices,
        selected_cluster_metrics = selected_cluster_metrics,
        clustering_methods = clustering_methods
      )
    }, error = function(e) {
      warning("Cluster quality calculation failed for dataset '", dataset_name, "' with error: ", e$message)
      next
    })

    # Create plots
    message("Creating plots...")
    tryCatch({
      plots <- create_dataset_plots(
        clustering_results = clustering_results,
        projection_methods = local_projection_methods,
        clustering_methods = clustering_methods,
        cluster_number_methods = cluster_number_methods,
        dataset_name = dataset_name,
        label_points = label_points,
        highlight_misclassified = highlight_misclassified,
        points_colored_for = points_colored_for,
        cells_colored_for = cells_colored_for,
        palette_target = palette_target,
        palette_cluster = palette_cluster,
        nProc = nProc
      )

      # Highlight best clustering if required
      if (highlight_best_clustering) {
        
      best_combination_rank <- max(cluster_quality_results[[dataset_name]][["combined_rank"]] )
      if (length(unique(cluster_quality_results[[dataset_name]][["combined_rank"]])) > 1) {
        second_best_combination_rank <- sort(unique(cluster_quality_results[[dataset_name]][["combined_rank"]]) , decreasing = TRUE)[2]
      } else {
        second_best_combination_rank <- -1
      }
      best_combination <- 
          cbind.data.frame( 
            FirstSecond = 1,  
            cluster_quality_results[[dataset_name]][cluster_quality_results[[dataset_name]]$combined_rank == best_combination_rank, c("projection_method", "clustering_method", "cluster_number_method")] 
        )
      if (second_best_combination_rank > 0) {
        second_best_combination <-
      cbind.data.frame( 
        FirstSecond = 2,  
        cluster_quality_results[[dataset_name]][cluster_quality_results[[dataset_name]]$combined_rank == second_best_combination_rank, c("projection_method", "clustering_method", "cluster_number_method")] 
      )
        best_second_best_combination <- rbind.data.frame(
          best_combination,
          second_best_combination
        )
      }
        
      best_second_best_combination <- best_second_best_combination[!duplicated(best_second_best_combination)]

      for (i in seq_len(nrow(best_second_best_combination))) {
        if (best_second_best_combination$FirstSecond[i] == 1) {
          plots[[best_second_best_combination[i, "projection_method"]]][[best_second_best_combination[i, "clustering_method"]]][[best_second_best_combination[i, "cluster_number_method"]]] <- 
            suppressMessages(suppressMessages(plots[[best_second_best_combination[i, "projection_method"]]][[best_second_best_combination[i, "clustering_method"]]][[best_second_best_combination[i, "cluster_number_method"]]] +
            scale_color_hue() +
            scale_fill_hue()
            ))
      }
        if (best_second_best_combination$FirstSecond[i] == 2) {
          plots[[best_second_best_combination[i, "projection_method"]]][[best_second_best_combination[i, "clustering_method"]]][[best_second_best_combination[i, "cluster_number_method"]]] <- 
            suppressMessages(suppressMessages(plots[[best_second_best_combination[i, "projection_method"]]][[best_second_best_combination[i, "clustering_method"]]][[best_second_best_combination[i, "cluster_number_method"]]] +
            scale_color_viridis_d() +
            scale_fill_viridis_d()
            ))
        }
      }  
      }
      
      projections_plots[[dataset_name]] <- unlist(plots, recursive = FALSE)

    }, error = function(e) {
      warning("Plot creation failed for dataset '", dataset_name, "' with error: ", e$message)
      next
    })
  } # End of dataset loop

  # Final return
  return(list(
    projections_plots = projections_plots,
    projection_results = all_projection_results,
    clustering_results = all_clustering_results,
    cluster_quality_results = cluster_quality_results
  ))
}

############### Functions for data preparations ###############

# Function to prepare datasets
prepare_dataset <- function( input_X, Target = NULL, Label = NULL ) {
  input_X <- if ( is.matrix( input_X ) ) as.data.frame( input_X ) else input_X
  if ( !is.data.frame( input_X ) && !is.matrix( input_X ) ) stop( "Input must be a data frame or matrix." )

  # Use existing "Target" column if Target is NULL and "Target" exists in input_X
  output_Y <- if ( is.null( Target ) ) {
    if ( !"Target" %in% colnames( input_X ) ) {
      message( "'Target' is missing, creating 'Target' = 1." )
      rep( 1, nrow( input_X ) )
    } else {
      input_X$Target
    }
  } else {
    Target
  }

  # Use existing "Label" column if Labels is NULL and "Target" exists in input_X
  output_L <- if ( is.null( Label ) ) {
    if ( !"Label" %in% colnames( input_X ) ) {
      message( "Taking row names as case labels." )
      rownames( input_X )
    } else {
      message( "Taking 'Label' column as case labels." )
      input_X$Label
    }
  } else {
    if ( length( Label ) != nrow( input_X ) ) {
      message( "Length of 'Label' does not match numbr of rows in input matrix\nTaking row names as case labels." )
      rownames( input_X )
    } else {
      Label
    }
  }


  data_frame <- as.data.frame( input_X )
  if ( ncol( data_frame ) < 3 ) stop( "Matrix needs at least three columns including 'Target'." )

  # Ensure "Target" is numeric after checking availability
  data_frame$Target <- output_Y
  if ( !is.numeric( data_frame$Target ) ) {
    data_frame$Target <- as.numeric( factor( data_frame$Target ) )
  }

  # Add "Label" is numeric after checking availability
  data_frame$Label <- output_L

  return( data_frame )
}


# Function to retrieve datasets
retrieve_dataset <- function( dataset_name ) {
  if ( !exists( dataset_name, envir = .GlobalEnv ) ) {
    warning( "Dataset ", dataset_name, " not found. Skipping." )
    return( NULL )
  }

  data_set <- get( dataset_name, envir = .GlobalEnv )
  if ( !is.data.frame( data_set ) ) {
    warning( "Dataset ", dataset_name, " is not a data frame. Skipping." )
    return( NULL )
  }

  if ( !"Target" %in% names( data_set ) ) {
    warning( "No 'Target' column found in dataset ", dataset_name, ". Skipping." )
    return( NULL )
  }

  return( data_set )
}

# Combine results for plotting
create_combined_result <- function( projected_data, clusters, projection_result ) {
  combined_result <- cbind.data.frame(
    projected_data,
    Cluster = clusters,
    Target = projection_result$UniqueData$Target,
    Label = projection_result$UniqueData$Label
  )
  combined_result$Misclassified <- ifelse( combined_result$Cluster == combined_result$Target, 0, 1 )
  return( combined_result )
}


############### Functions for data projection ###############

# Function to generate projections
generate_projections <- function( data, projection_methods, seed = 42, jitter_one_dimensional_projections = FALSE , nProc = 2) {
  data_clean <- data[, !( names( data ) %in% c( "Target", "Label" ) )]
  target <- data$Target
  labels <- data$Label

  projections <- pbmcapply::pbmclapply( projection_methods, function( projection_method ) {
    message( "Applying projection: ", projection_method )
    tryCatch( {
      projection <- performProjection( X = data_clean, projection_method = projection_method, Target = target, labels = labels, seed = seed,
                                       jitter_one_dimensional_projections = jitter_one_dimensional_projections )
      if ( !is.list( projection ) || !"Projected" %in% names( projection ) )
        stop( "Invalid projection result." )
      return( projection )
    }, error = function( err ) {
      message( "Error with method ", projection_method, ": ", err$message )
      return( NULL )
    } )
  }, mc.cores = min( length( projection_methods ), nProc ) )

  names( projections ) <- projection_methods
  return( projections )
}


# Function to perform projections
performProjection <-
  function(X,
           projection_method = "PCA",
           scaleX = TRUE,
           Target = NULL,
           switchDims = FALSE,
           labels = NULL,
           seed = 42,
           jitter_one_dimensional_projections = FALSE) {

    # Check required libraries
    required_packages <- c("FactoMineR", "fastICA", "MASS", "Rtsne", "RDRToolbox", "mixOmics", "ABCanalysis")
    for (pkg in required_packages) {
      if (!requireNamespace(pkg, quietly = TRUE)) {
        stop(paste("Package", pkg, "is required but not installed. Please install it."))
      }
    }

    # Define valid methods
    valid_projection_methods <- c("none", "PCA", "ICA", "MDS", "tSNE", "isomap", "LDA",
                                  "PLSDA", "PLSLDA", "IPCA", "Umap", "NMF", "LLE", "autoencoder")
    # Validate selected projection method
    if (!(projection_method %in% valid_projection_methods)) {
      stop(paste("Invalid projection method. Choose from:", paste(valid_projection_methods, collapse = ", ")))
    }

    # Validate inputs
    if (!is.data.frame(X) && !is.matrix(X)) stop("X must be a data frame or matrix.")
    if (is.null(X) || nrow(X) == 0) stop("X must not be NULL or empty.")
    if (!is.null(Target) && length(Target) != nrow(X)) stop("Target length must match the number of rows in X.")
    if (!is.null(labels) && length(labels) != nrow(X)) stop("Labels length must match the number of rows in X.")

    # Define helper function to check integer(0)
    is.integer0 <- function(x) {
      is.integer(x) && length(x) == 0L
    }

    # Handle duplicates
    dupl <- which(duplicated(X))
    if (!is.null(Target) && projection_method != "PCA" && !is.integer0(dupl)) {
      Target <- Target[-dupl]
    }
    if (!is.null(labels) && projection_method != "PCA" && !is.integer0(dupl)) {
      labels <- labels[-dupl]
    }
    if (projection_method != "PCA") X <- X[!duplicated(X), ]

    # Scale data if requested
    if (scaleX && !projection_method %in% c("IPCA", "NMF")) {
      X <- scale(X)
    }

    # Assign labels (if not provided)
    if (is.null(labels)) {
      if (!is.null(rownames(X))) {
        labels <- rownames(X)
      } else {
        labels <- seq_len(nrow(X))
      }
    }

    # Combine X and Target into a single data frame
    if (!is.null(Target)) {
      df_unique <- data.frame(X, Target = Target, Label = labels)
    } else {
      df_unique <- data.frame(X, Label = labels)
    }

    # Separate X and Target
    X_unique <- df_unique[, setdiff(names(df_unique), c("Target", "Label")), drop = FALSE]
    if (!is.null(Target)) {
      Target_unique <- df_unique$Target
    }

    
    # Estimate the dimensionality of the data set
    get_recommended_dimensions <- function(data, variance_threshold = 0.8, correlation_threshold = 0.7) {
      # Remove non-numeric columns
      numeric_data <- data 
      
      # Perform PCA
      # pca1 <- FactoMineR::PCA( X, graph = FALSE )
      # 
      # # Variance explained criterion
      # 
      # var_dims <- min(which(factoextra::get_eig(pca1)[,"cumulative.variance.percent"] >= 100*variance_threshold))
      # 
      # # Kaiser - Gutman criterion
      # 
      # kaiser_dims <- max( 2, sum( pca1$eig[, 1] > 1 ) )
      
      pca_result <- prcomp(numeric_data, scale. = TRUE)

      # Variance explained criterion
      var_explained <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
      var_dims <- which(var_explained >= variance_threshold)[1]

      # Kaiser criterion
      eigenvalues <- pca_result$sdev^2
      kaiser_dims <- sum(eigenvalues > 1)
      
      # Correlation analysis
      cor_matrix <- cor(numeric_data)
      high_cor_count <- sum(abs(cor_matrix[upper.tri(cor_matrix)]) > correlation_threshold)
      cor_dims <- ncol(numeric_data) - high_cor_count
      
      # Final recommendation (taking the average and rounding up)
      recommended_dims <- max(2,ceiling(mean(c(var_dims, kaiser_dims, cor_dims))))
      
      return(recommended_dims)
    }
    
    # Get dimensions
    ncomp <- get_recommended_dimensions(X_unique)
    
    
    # Validate Target for methods that require it
    if (projection_method == "PLSDA" && is.null(Target)) {
      stop("Projection method 'PLSDA' requires Target to be provided.")
    }

    # Seed setting for reproducibility
    set.seed(seed)

    # Perform projection
    Proj <- NULL
    switch(projection_method,

      "none" = {
        # No projection, use the data as is
        Proj <- X_unique
      },
      "PCA" = {
        # Perform Principal Component Analysis
        Proj.pca <- FactoMineR::PCA( X_unique, graph = FALSE, ncp = ncomp )
        Proj <- Proj.pca$ind$coord
      },
      "ICA" = {
        # Helper fucntion to sort ICA componets for importance
        sort_ica_components <- function( S, A ) {
          # S: Source matrix (ICA components)
          # A: Mixing matrix
          # Calculate the variance explained by each componentQ
          var_explained <- apply( S, 2, var )

          # Sort the components based on variance explained
          order_components <- order( var_explained, decreasing = TRUE )

          # Reorder the columns of S and A
          S_sorted <- S[, order_components]
          A_sorted <- A[, order_components]

          return( list( S = S_sorted, A = A_sorted ) )
        }

        # Run ICA
        Proj.ica <- fastICA::fastICA( X_unique,
                                      n.comp = ncomp,
                                      alg.typ = "parallel", fun = "logcosh", alpha = 1,
                                      method = "C", row.norm = FALSE, maxit = 1000,
                                      tol = 1e-04, verbose = FALSE
        )

        # Sort ICA components so that the first two are most relevant
        Proj.ica_sorted <- sort_ica_components( S = Proj.ica$S, A = Proj.ica$A )
        Proj <- Proj.ica_sorted$S
      },
      "MDS" = {
        k_estimate <- max( 2, ncol( X_unique ) )
        # Perform MDS with maximum dimensions
        # Multidimensional Scaling
        d <- dist( X_unique )

        # Repeat MDS with a better estimate for k
        mds.fit <- MASS::isoMDS( d, k = ncomp, maxit = 1000, tol = 1e-4 )
        Proj <- mds.fit$points
      },
      "tSNE" = {
        # t-Distributed Stochastic Neighbor Embedding
        perpl <- ifelse( nrow( X_unique ) / 3 - 1 / 3 < 30, nrow( X_unique ) / 3 - 0.34, 30 )
        Proj <- Rtsne::Rtsne( X_unique, perplexity = perpl )[[2]]
      },
      "isomap" = {
        # Repeat Isomap with a better estimate for dims
        Proj1 <- RDRToolbox::Isomap( as.matrix( X_unique ), dims = ncomp, k = 5 )
        Proj <- Proj1[[1]]
      },
      "PLSDA" = {
        # Partial Least Squares Discriminant Analysis
        res.plsda <- mixOmics::plsda( X_unique, factor( Target_unique ) )
        res.plsda.plot <- mixOmics::plotIndiv( res.plsda )
        Proj <- res.plsda.plot$df[c( "x", "y" )]
      },
      "LDA" = {
        # Run Linear Discriminant Analysis
        dfLDA <- cbind.data.frame( Target_unique = Target_unique, X_unique )
        res.lda <- MASS::lda( factor( Target_unique ) ~ ., data = dfLDA, dimen = ncomp,method = "t")
        lda_projection <- predict.lda( res.lda, X_unique, dimen = ncomp )
        Proj <- lda_projection$x
      },
      "PLSLDA" = {
        # Function to perform PLS-LDA projection
        pls_lda_projection <- function( X, y, ncomp ) {

          dfPLSLDA <- cbind.data.frame( y = y, X )
          # Perform PLS


          pls_model <- pls::plsr( y ~ ., data = dfPLSLDA, ncomp = ncomp, scale = TRUE )

          # Extract PLS scores
          pls_scores <- pls::scores( pls_model )[, 1:ncomp]

          # Fit LDA on PLS scores
          lda_model <- MASS::lda( pls_scores, grouping = y )

          # Project PLS scores onto LDA space
          lda_projection <- predict( lda_model, newdata = pls_scores )$x

          return( list( pls_model = pls_model,
                        lda_model = lda_model,
                        pls_scores = pls_scores,
                        lda_projection = lda_projection ) )
        }

        results.plslda <- pls_lda_projection( X = as.data.frame( X_unique ), y = Target_unique, ncomp = ncomp )

        Proj <- results.plslda$lda_projection
      },
      "IPCA" = {
        # Independent Principal Component Analysis with potential dimension switch
        res.ipca <- mixOmics::ipca( as.matrix( X_unique ),
                                    ncomp = ncomp,
                                    mode = "deflation",
                                    fun = "logcosh",
                                    scale = F,
                                    w.init = NULL,
                                    max.iter = 1000,
                                    tol = 1e-04
        )
        res.ipca.plot <- mixOmics::plotIndiv( res.ipca )
        Proj <- res.ipca.plot$df[c( "x", "y" )]
        
        # Conditionally switch dimensions if needed
        if ( switchDims ) {
          if ( res.ipca$prop_expl_var$X[1] < res.ipca$prop_expl_var$X[2] ) {
            Proj <- res.ipca.plot$df[c( "y", "x" )]
          }
        }
      },
      "Umap" = {
        # UMAP projection
        res.umap <- umap::umap( X_unique )
        Proj <- res.umap$layout
      },
      "NMF" = {
        # Non-negative Matrix Factorization 
        # Function for Sequential NMF with non-negativity check
        perform_nmf <- function( data, ncomp, threshold = 0.9 ) {
          # Check for non-positive values
          if ( any( data <= 0 ) ) {
            message( "Input matrix contains non-positive values. Shifting all values to be positive." )
            min_value <- min( data )
            # Shift the data to make all values positive
            data <- data - min_value + 1e-10 # Small constant to ensure all values are positive
          }

          nmf_result <- NMF::nmf( data, ncomp )

          # Use coefficient matrix directly; no need to transpose for this case.
          coef_matrix <- NMF::basis( nmf_result )

          # Each row corresponds to a sample and columns are the components.
          projected_data <- coef_matrix

          return(projected_data)
        }

        Proj <- suppressMessages( perform_nmf( data = as.matrix( X_unique ), ncomp = ncomp ) )
      },
      "LLE" = {
# Local linear embedding
        # lle_with_fslle <- function(X, d, k_max = 30, tol = 1e-3) {
        #   n <- nrow(X)
        #   p <- ncol(X)
        #   
        #   # FSLLE: Function to calculate spatial correlation index
        #   calculate_sci <- function(X, k) {
        #     nn <- RANN::nn2(X, k = min(k + 1, nrow(X)))
        #     neighbors <- nn$nn.idx[, -1, drop = FALSE]
        #     
        #     local_diff <- X[neighbors, , drop = FALSE] - X[rep(1:nrow(X), each = ncol(neighbors)), ]
        #     local_var <- apply(local_diff, 1, var)
        #     local_mean_diff <- rowMeans(abs(local_diff))
        #     
        #     sci <- mean(local_var) / mean(local_mean_diff)
        #     return(sci)
        #   }
        #   
        #   
        #   # FSLLE: Find optimal k
        #   k_values <- 2:k_max
        #   sci_values <- sapply(k_values, function(k) calculate_sci(X, k))
        #   optimal_k <- k_values[which.min(sci_values)]
        #   
        #   cat("Optimal k determined by FSLLE:", optimal_k, "\n")
        #   
        #   # LLE algorithm with optimal k
        #   nn <- RANN::nn2(X, k = optimal_k + 1)
        #   neighbors <- nn$nn.idx[, -1]
        #   
        #   W <- Matrix::Matrix(0, n, n, sparse = TRUE)
        #   for (i in 1:n) {
        #     Xi <- matrix(X[i,], nrow = optimal_k, ncol = p, byrow = TRUE)
        #     Z <- X[neighbors[i,], ] - Xi
        #     C <- tcrossprod(Z)
        #     C <- C + diag(tol * sum(diag(C)), optimal_k)  # Regularization
        #     w <- solve(C, rep(1, optimal_k))
        #     w <- w / sum(w)
        #     W[i, neighbors[i,]] <- w
        #   }
        #   
        #   M <- diag(n) - W - t(W) + tcrossprod(W)
        #   eig_result <- eigen(M)
        #   
        #   sorted_indices <- order(eig_result$values)
        #   eigenvectors <- eig_result$vectors[, sorted_indices]
        #   
        #   Y <- eigenvectors[, 2:(d+1)]
        #   
        #   importance <- 1 - eig_result$values[sorted_indices[2:(d+1)]] / sum(eig_result$values)
        #   importance <- importance / sum(importance)
        #   
        #   result <- list(
        #     projection = Y,
        #     importance = importance,
        #     optimal_k = optimal_k
        #   )
        #   
        #   return(result)
        # }
        # 
        # result.LLE <- lle_with_fslle(as.matrix( X_unique ), d = ncomp)  # Reduce to 3 dimensions
        # Proj <- result.LLE$projection
        
        lle_optimized <- function(data, max_dim = 10, max_k = 30, perplexity = 30) {
          # Ensure data is a numeric matrix
          X <- as.matrix(data)
          X <- scale(X) # Standardize the data
          
          lle <- function(X, k, d, reg = 1e-3) {
            n <- nrow(X)
            p <- ncol(X)
            
            # Find k nearest neighbors
            nn <- RANN::nn2(X, k = k + 1)
            neighbors <- nn$nn.idx[, -1]
            
            # Compute weights
            W <- matrix(0, n, n)
            for (i in 1:n) {
              z <- X[neighbors[i,], , drop = FALSE] - matrix(X[i,], k, p, byrow = TRUE)
              C <- tcrossprod(z)
              C <- C + diag(k) * reg * sum(diag(C)) # Regularization
              w <- solve(C, rep(1, k))
              w <- w / sum(w)
              W[i, neighbors[i,]] <- w
            }
            
            # Compute embedding
            M <- diag(n) - W - t(W) + t(W) %*% W
            eig <- eigen(M, symmetric = TRUE)
            Y <- eig$vectors[, (n - d):(n - 1), drop = FALSE]
            
            # Calculate explained variance for each dimension
            var_explained <- apply(Y, 2, var)
            
            # Sort dimensions by explained variance (descending order)
            order <- order(var_explained, decreasing = TRUE)
            Y <- Y[, order, drop = FALSE]
            
            return(list(Y = Y, var_explained = var_explained[order]))
          }
          
          optimize_params <- function(X, max_k, max_d, perplexity) {
            n <- nrow(X)
            scores <- matrix(Inf, max_k, max_d)
            
            # Compute t-SNE embedding for reference
            tsne_emb <- Rtsne::Rtsne(X, dims = 2, perplexity = perplexity, verbose = FALSE)$Y
            
            for (k in seq(5, max_k, by = 5)) {
              # Step by 5 for efficiency
              for (d in 1:min(max_d, k - 1)) {
                tryCatch({
                  result <- lle(X, k, d)
                  Y <- result$Y
                  
                  # Ensure Y has at least 2 columns for KL divergence calculation
                  if (ncol(Y) < 2) {
                    Y <- cbind(Y, rep(0, nrow(Y)))
                  }
                  
                  # Compute KL divergence between t-SNE and LLE embeddings
                  kl_div <- KL_divergence(tsne_emb, Y[, 1:2, drop = FALSE])
                  
                  # Compute trustworthiness
                  trust <- trustworthiness(X, Y, k = min(10, k - 1))
                  
                  # Combine scores (lower is better)
                  scores[k, d] <- kl_div - trust
                  
                }, error = function(e) {
                  message("Error for k=", k, ", d=", d, ": ", e$message)
                })
              }
            }
            
            best <- which(scores == min(scores, na.rm = TRUE), arr.ind = TRUE)[1,]
            return(list(k = best[1], d = best[2]))
          }
          
          # Helper functions
          KL_divergence <- function(P, Q) {
            P <- as.matrix(dist(P))
            Q <- as.matrix(dist(Q))
            P <- P / sum(P)
            Q <- Q / sum(Q)
            return(sum(P * log((P + 1e-10) / (Q + 1e-10))))
          }
          
          trustworthiness <- function(X, Y, k) {
            n <- nrow(X)
            r_x <- apply(as.matrix(dist(X)), 2, rank)
            r_y <- apply(as.matrix(dist(Y)), 2, rank)
            sum_penalty <- sum(sapply(1:n, function(i) {
              sum(pmax(r_x[i, r_y[i,] <= k] - k, 0))
            }))
            return(1 - 2 / (n * k * (2 * n - 3 * k - 1)) * sum_penalty)
          }
          
          params <- optimize_params(X, max_k, max_dim, perplexity)
          params$d <- max(2, params$d)
          result <- lle(X, params$k, params$d)
          
          return(list(projection = result$Y, k = params$k, d = params$d, var_explained = result$var_explained))
        }
        
        result.LLE <- lle_optimized( X_unique )
        Proj <- result.LLE$projection
        #Proj <- apply(Proj,2,jitter)
      },
      "autoencoder" = {
        # Function for a simple autoencoder
        AutoEncode2 <- function(Data, Hidden = NULL, Learningrate = c(0.01, 0.1, 0.5), 
                                Threshold = c(0.01, 0.1, 0.5), n_times_n_features = 3,
                                Epochs = 10000, ActivationFunction = c("logistic", "tanh"), ...) {
          # Load required libraries
          if (!requireNamespace("neuralnet", quietly = TRUE)) {
            stop("The 'neuralnet' package is required but not installed.")
          }
          
          # Check for numeric data and no missing values
          if (!is.numeric(Data) || any(is.na(Data))) {
            stop("Data must be numeric and contain no missing values.")
          }
          
          # Validate hidden layers
          if (!is.null(Hidden) && (length(Hidden) %% 2) != 1)
            stop("AutoEncode: Number of Layers of Hidden Neurons has to be odd")
          
          # Define the number of features and samples
          n_features <- ncol(Data)
          n_samples <- nrow(Data)
          
          # Default hidden layer structure if not provided
          if (is.null(Hidden)) {
            bottleneck_size <- max(2, round(n_times_n_features * n_features))
            middle_layer <- bottleneck_size / 2
            Hidden <- c(bottleneck_size, middle_layer, bottleneck_size)
          }
          
          d <- as.data.frame(Data)
          formel <- paste0(paste(colnames(d), collapse = " + "), " ~ ",
                           paste(colnames(d), collapse = " + "))
          
          # Define parameter grid for optimization
          param_grid <- expand.grid(
            Learningrate = Learningrate,
            Threshold = Threshold
          )
          
          # Function to train and evaluate the model
          train_model <- function(Learningrate, Threshold, ActivationFunction) {
            tryCatch({
              net <- neuralnet::neuralnet(formel, d, hidden = Hidden, stepmax = Epochs,
                                          lifesign = "none", linear.output = FALSE,
                                          algorithm = "backprop", learningrate = Learningrate,
                                          threshold = Threshold)
              
              x <- neuralnet::compute(net, d)
              reproduction <- x$net.result
              mse <- mean((as.matrix(Data) - reproduction)^2)
              return(list(net = net, mse = mse))
            }, error = function(e) {
              return(list(net = NULL, mse = Inf))
            })
          }
          
          # Perform grid search
          results <- apply(param_grid, 1, function(params) {
            train_model(params[1], params[2], as.character(params[3]))
          })
          
          # Find the best model
          best_model <- results[[which.min(sapply(results, function(x) x$mse))]]
          
          if (is.null(best_model$net)) {
            stop("Failed to train any valid model. Try adjusting the parameter ranges.")
          }
          
          net <- best_model$net
          x <- neuralnet::compute(net, d)
          ProjectionLayer <- ceiling((length(Hidden) + 2) / 2)
          nrOfDim <- ncol(x$neurons[[ProjectionLayer]]) - 1
          projection <- x$neurons[[ProjectionLayer]][, 2:(nrOfDim + 1), drop = FALSE]
          
          # Order dimensions by importance (variance)
          var_importance <- apply(projection, 2, var)
          ordered_projection <- projection[, order(var_importance, decreasing = TRUE), drop = FALSE]
          
          return(ordered_projection)
        }
        Proj <- AutoEncode2(as.matrix(X_unique), 
                            Learningrate = c(0.01, 0.1, 0.5),
                            Threshold = c(0.01, 0.1, 0.5),
                            ActivationFunction = c("logistic", "tanh"),
                            Epochs = 5000,
                            n_times_n_features = 3)
      }, {
      # No projection, use the data as is
      Proj <- X_unique
    }
    )

    # Ensure that at least two columns are retunred 
    if ( ncol( Proj ) == 1 ) {
      if ( jitter_one_dimensional_projections ) {
        Proj <- cbind.data.frame( Proj, jitter( Proj ) )
      } else {
        Proj <- cbind.data.frame( Proj, Proj )
      }
    }
    
    Proj <- data.frame(Proj)
    names(Proj) <- paste0("Dim", 1:ncol(Proj))

    # Return the projection and unique data frame
    return( list( Projected = Proj, UniqueData = df_unique ) )
  }


############### Functions for clustering ###############

# Apply clustering algorithms
apply_clustering <- function( projection_results, clustering_methods, cluster_number_methods, method_for_hcpc, 
                              distance_metric = "euclidean", seed = 42, nProc = 2) {

  cluster_list <- pbmcapply::pbmclapply( names( projection_results ), function( projection ) {
    projection_result <- projection_results[[projection]]
    tryCatch( {
      if ( is.null( projection_result$UniqueData$Target ) ) {
        stop( "Required data is missing for clustering." )
      }
      projected_data <- as.data.frame( projection_result$Projected )
      clusters_per_projection <- lapply( clustering_methods, function( cluster_alg ) {
        clusters_results <- lapply( cluster_number_methods, function( cluster_number_method ) {
          set.seed( seed )
          clusters <- performClustering( X = projected_data, Target = projection_result$UniqueData$Target,
                                         clustering_method = cluster_alg, cluster_number_method = cluster_number_method,
                                         method_for_hcpc = method_for_hcpc,
                                         distance_metric = distance_metric )
          if (cluster_alg != "none") clusters <- renameClusters( trueCls = projection_result$UniqueData$Target, clusters )
          combined_result <- create_combined_result( projected_data, clusters, projection_result )
          return( combined_result )
        } )
        names( clusters_results ) <- cluster_number_methods
        return( clusters_results )
      } )
      names( clusters_per_projection ) <- clustering_methods
      return( clusters_per_projection )

    }, error = function( err ) {
      message( "Error in clustering with projection ", projection, ": ", err$message )
      return( NULL )
    } )
  }, mc.cores = min( length( projection_methods ), nProc ) )

  names( cluster_list ) <- names( projection_results )
  return( cluster_list )
}


# Function to determine the number of clusters
findOptimalClusters <- function( X, clustering_method, cluster_number_method, nProc = 2, Target = NULL,
                                 method_for_hcpc = "ward", distance_metric = "euclidean" ) {

  # Load necessary libraries
  if ( !requireNamespace( "NbClust", quietly = TRUE ) ) {
    stop( "Package 'NbClust' is required but not installed." )
  }
  if ( !requireNamespace( "FactoMineR", quietly = TRUE ) ) {
    stop( "Package 'FactoMineR' is required but not installed." )
  }
  if ( !requireNamespace( "parallel", quietly = TRUE ) ) {
    stop( "Package 'parallel' is required but not installed." )
  }

  # Function to calculate mode
  calculateMode <- function( v ) {
    if ( length( v ) == 0 ) return( NA )
    freqTable <- table( v )
    modes <- as.numeric( names( freqTable )[freqTable == max( freqTable )] )
    if ( length( modes ) > 1 ) modes else modes[1]
  }

  nClusters <- switch( cluster_number_method,
                       "NbClust" = {
                         # Adjust clustering method for compatibility with NbClust
                         clustering_method <- switch( clustering_method,
                                                      "kmedoids" = "kmeans",
                                                      "diana" = "ward.D2",
                                                      "hcpc" = "ward.D2",
                                                      clustering_method
                         )

                         # Try running NbClust using parallel processing
                         nbclustIndices <- c( "kl", "ch", "hartigan", "ccc", "scott", "marriot", "trcovw", "tracew",
                                              "friedman", "rubin", "cindex", "db", "silhouette", "duda", "pseudot2",
                                              "beale", "ratkowsky", "ball", "ptbiserial", "gap", "frey", "mcclain",
                                              "gamma", "gplus", "tau", "dunn", "hubert", "sdindex", "dindex", "sdbw" )

                         # Determine number of clusters by running NbClust method by method
                         nClustersTest <- parallel::mclapply( nbclustIndices, function( i ) {
                           res <- try( suppressWarnings( NbClust::NbClust( data = X, diss = NULL, distance = distance_metric,
                                                                           min.nc = 2, max.nc = 12, method = clustering_method, index = i ) ), silent = TRUE )
                           if ( !inherits( res, "try-error" ) ) {
                             length( unique( res$Best.partition ) )
                           } else {
                             warning( paste( "Error with index", i ) )
                             NA
                           }
                         }, mc.cores = nProc )

                         # Compute mode of results to determine optimal number of clusters
                         nC <- na.omit( unlist( nClustersTest ) )
                         result <- if ( length( nC ) > 0 ) calculateMode( nC ) else NA
                         result
                       },
                       "hcpc" = {
                         res.hcpc <- FactoMineR::HCPC( X, consol = TRUE, method = method_for_hcpc, metric = distance_metric, nb.clust = -1,
                                                       iter.max = 100, graph = FALSE, nstart = 100 )
                         length( unique( res.hcpc$data.clust$clust ) )
                       },
                       # Default case if Target is provided or none matches
                       if ( !is.null( Target ) ) {
                         length( unique( Target ) )
                       } else {
                         1
                       }
  )

  if ( is.na( nClusters ) ) {
    warning( "Unable to determine the number of clusters. Setting to 1 or the original classes if provided." )
    if ( !is.null( Target ) ) {
      nClusters <- length( unique( Target ) )
    } else {
      nClusters <- 1
    }
  }

  return( nClusters )
}


# Function to perform clustering
performClustering <- function( X, Target = NULL, clustering_method = "kmeans", cluster_number_method = "orig", method_for_hcpc = "ward",
                               distance_metric = "euclidean" ) {

  # Check if input X is a non-empty data frame
  if ( !is.data.frame( X ) || nrow( X ) == 0 ) stop( "Input data must be a non-empty data frame." )

  # Define valid clustering methods
  valid_clustering_methods <- c( "none", "kmeans", "kmedoids",
                                 "diana", "hcpc", "ward.D2", "single", "average", "median", "complete", "centroid" )
  valid_hcpc_methods <- c( "average", "single", "complete", "ward", "weighted", "flexible", "gaverage" )
  valid_distances <- c( "euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski" )
  valid_cluster_number_methods <- c( "orig", "NbClust", "hcpc" )

  # Validate the selected method
  if ( !( clustering_method %in% valid_clustering_methods ) )
    stop( paste( "Invalid clustering method. Choose from:", paste( valid_clustering_methods, collapse = ", " ) ) )
  if ( !( method_for_hcpc %in% valid_hcpc_methods ) )
    stop( paste( "Invalid method for HCPC clustering. Choose from:", paste( valid_hcpc_methods, collapse = ", " ) ) )
  if ( !( distance_metric %in% valid_distances ) )
    stop( paste( "Invalid distance metric. Choose from:", paste( valid_distances, collapse = ", " ) ) )
  if ( !( cluster_number_method %in% valid_cluster_number_methods ) )
    stop( paste( "Invalid method for cluster number determination. Choose from:", paste( valid_cluster_number_methods, collapse = ", " ) ) )

  # If Target is NULL, initialize it with default values
  if ( is.null( Target ) ) Target <- rep( 1, nrow( X ) )

  # Determine the number of clusters using the custom function

  if ( clustering_method == "none" ) {
    nClusters <- length( unique( Target ) )
  } else {
    nClusters <- findOptimalClusters( X, clustering_method = clustering_method,
                                      cluster_number_method = cluster_number_method, nProc = nProc, Target = Target )
  }

  # Perform clustering based on the specified method
  cluster_result <- switch( clustering_method,
    "none" = Target, # Return Target directly if method is "none"
    "kmeans" = {
      # Perform K-means clustering
      kmeans( X, centers = nClusters, nstart = 100, )$cluster
    },
    "kmedoids" = {
      # Perform K-medoids clustering using PAM
      cluster::pam( X, k = nClusters )$clustering
    },
    "diana" = {
      # Perform divisive clustering
      cluster::diana( X )$clustering
    },
    "hcpc" = {
      # Perform hierarchical clustering and principal component analysis
      res.pca <- FactoMineR::PCA( X, graph = FALSE )
      FactoMineR::HCPC( res.pca, nb.clust = nClusters, metric = distance_metric,
                        graph = FALSE, nstart = 100, method = method_for_hcpc )$
        data.clust$
        clust
    }, {
    # Perform other hierarchical clustering methods
    clusterObject <- stats::hclust( dist( X, method = distance_metric ), method = clustering_method )
    as.numeric( cutree( clusterObject, k = nClusters ) )
  } )

  # Return the clustering result
  return( cluster_result )
}


# Function to align cluster names with likely class labels of the prior classification
renameClusters <- function( trueCls, currentCls, K = 12 ) {

  # Helper function to reduce clusters
  ReduceClsToK <- function( Cls, K = 12 ) {
    uniqueCls <- unique( Cls )
    while ( length( uniqueCls ) > K ) {
      counts <- table( Cls )
      to_merge <- names( sort( counts )[1:2] )
      Cls[Cls %in% to_merge] <- to_merge[1]
      uniqueCls <- unique( Cls )
    }
    return( Cls )
  }

  # Preprocess input
  trueCls[!is.finite( trueCls )] <- 9999
  currentCls[!is.finite( currentCls )] <- 9999

  # Warnings
  if ( length( unique( trueCls ) ) > 9 ) {
    warning( "Too many clusters in PriorCls. Consider using cloud computing." )
  }
  if ( length( unique( currentCls ) ) > K ) {
    warning( "Too many clusters in CurrentCls. Combining smallest clusters." )
    currentCls <- ReduceClsToK( currentCls, K )
  }

  # Normalize clusters
  trueCls <- as.numeric( factor( trueCls ) )
  currentCls <- as.numeric( factor( currentCls ) )

  # Get unique labels
  uniqueLabels <- sort( unique( c( trueCls, currentCls ) ) )
  nLabels <- length( uniqueLabels )

  # Generate permutations
  permutations <- combinat::permn( nLabels )

  # Find best permutation
  bestAccuracy <- 0
  bestPermutation <- NULL

  for ( perm in permutations ) {
    newLabels <- perm[match( currentCls, seq_along( perm ) )]
    accuracy <- sum( trueCls == newLabels ) / length( trueCls )
    if ( accuracy > bestAccuracy ) {
      bestAccuracy <- accuracy
      bestPermutation <- perm
    }
  }

  # Rename clusters
  renamedCls <- bestPermutation[match( currentCls, seq_along( bestPermutation ) )]

  return( renamedCls )
}


# Function to calculate Adjusted Rand Index (ARI) without using mclust
calculate_adjusted_rand_index <- function( Target, Clusters ) {
  # Helper function to compute a contingency table
  contingency_table <- function( x, y ) {
    table( factor( x, levels = unique( c( x, y ) ) ), factor( y, levels = unique( c( x, y ) ) ) )
  }

  # Compute the contingency table
  cont_table <- contingency_table( Target, Clusters )

  # Sum over rows and columns
  sum_comb_n <- function( n ) sum( choose( n, 2 ) )

  # Total number of pairs
  N <- sum( cont_table )

  # Sum of combinations for each cell
  sum_comb_c_ij <- sum( mapply( choose, as.numeric( cont_table ), 2 ) )

  # Sum of combinations for each row
  sum_comb_a_i <- sum_comb_n( rowSums( cont_table ) )

  # Sum of combinations for each column
  sum_comb_b_j <- sum_comb_n( colSums( cont_table ) )

  # Calculate ARI
  expected_index <- sum_comb_a_i * sum_comb_b_j / choose( N, 2 )
  max_index <- ( sum_comb_a_i + sum_comb_b_j ) / 2
  ari <- ( sum_comb_c_ij - expected_index ) / ( max_index - expected_index )

  return( ari )
}


# Function to calculate Dunn's Index using a distance matrix
calculate_dunn_index <- function( distance_matrix, clusters ) {
  # Convert the distance vector to a complete distance matrix
  distance_matrix <- as.matrix( distance_matrix )

  # Number of clusters
  unique_clusters <- unique( clusters )
  k <- length( unique_clusters )

  # Initialize variables for inter-cluster and intra-cluster distances
  min_inter_cluster_dist <- Inf
  max_intra_cluster_dist <- 0

  # Calculate inter-cluster distances
  for ( i in 1:( k - 1 ) ) {
    for ( j in ( i + 1 ):k ) {
      cluster_i_indices <- which( clusters == unique_clusters[i] )
      cluster_j_indices <- which( clusters == unique_clusters[j] )

      for ( p in cluster_i_indices ) {
        for ( q in cluster_j_indices ) {
          dist <- distance_matrix[p, q]
          if ( !is.na( dist ) && dist < min_inter_cluster_dist ) {
            min_inter_cluster_dist <- dist
          }
        }
      }
    }
  }

  # Calculate intra-cluster distances
  for ( i in 1:k ) {
    cluster_indices <- which( clusters == unique_clusters[i] )
    if ( length( cluster_indices ) > 1 ) {
      # Ensure there's more than one point in the cluster
      for ( p in 1:( length( cluster_indices ) - 1 ) ) {
        for ( q in ( p + 1 ):length( cluster_indices ) ) {
          dist <- distance_matrix[cluster_indices[p], cluster_indices[q]]
          if ( !is.na( dist ) && dist > max_intra_cluster_dist ) {
            max_intra_cluster_dist <- dist
          }
        }
      }
    }
  }

  # Ensure the max_intra_cluster_dist is valid for division
  if ( max_intra_cluster_dist == 0 ) {
    stop( "Intra-cluster distance is zero, which might indicate that each cluster has a single point." )
  }

  # Calculate Dunn's Index
  dunn_index <- min_inter_cluster_dist / max_intra_cluster_dist

  return( dunn_index )
}


# Calculate the Davies-Bold cluster index
calculate_davies_bouldin_index <- function( clusters, data ) {

  # Number of clusters
  k <- length( unique( clusters ) )

  # Ensure data is numeric
  numeric_data <- data[, sapply( data, is.numeric )]

  # Calculate centroids for each cluster
  centroids <- aggregate( numeric_data, by = list( cluster = clusters ), FUN = mean )
  rownames( centroids ) <- centroids$cluster
  centroids <- centroids[, -1] # Remove the cluster column

  # Calculate average distance within clusters
  avg_within_dist <- tapply( seq_len( nrow( data ) ), clusters, function( idx ) {
    cluster_data <- data[idx,]
    centroid <- centroids[as.character( clusters[idx[1]] ),]
    mean( apply( cluster_data, 1, function( x ) sqrt( sum( ( x - centroid )^2 ) ) ) )
  } )

  emicheps <- .Machine$double.eps^0.5

  # Calculate Davies-Bouldin index
  db_index <- mean( sapply( 1:k, function( i ) {
    max_ratio <- 0
    for ( j in 1:k ) {
      if ( i != j ) {
        dist_between <- sqrt( sum( ( centroids[i,] - centroids[j,] )^2 ) )
        ratio <- ( avg_within_dist[i] + avg_within_dist[j] ) / ( dist_between + emicheps )
        max_ratio <- max( max_ratio, ratio )
      }
    }
    max_ratio
  } ) )

  return( db_index )
}


# Calculate the Density-Based Clustering Validation (DBCV) index
calculate_dbcv <- function( data, clusters, dist_method = "euclidean" ) {

  # Calculate the DBCV index (using a simplified heuristic approach)
  intra_density <- numeric( max( clusters ) )
  inter_density <- numeric( max( clusters ) )

  for ( i in unique( clusters ) ) {
    cluster_points <- data[clusters == i,]
    if ( nrow( cluster_points ) > 1 ) {
      intra_cluster_dist_matrix <- dist( cluster_points, method = dist_method )
      intra_density[i] <- mean( intra_cluster_dist_matrix )
    }
  }

  for ( i in unique( clusters ) ) {
    for ( j in unique( clusters ) ) {
      if ( i < j ) {
        combined_points <- rbind( data[clusters == i,], data[clusters == j,] )
        inter_cluster_dist_matrix <- dist( combined_points, method = dist_method )
        inter_density[i] <- mean( inter_cluster_dist_matrix )
      }
    }
  }

  dbcv_index <- sum( intra_density ) / ( sum( intra_density ) + sum( inter_density ) )

  return( dbcv_index )
}


# Calinski-Harabasz Index
calculate_calinski_harabasz_index <- function( data, cluster_labels ) {
  n <- nrow( data )
  k <- length( unique( cluster_labels ) )

  # Calculate overall mean
  overall_mean <- colMeans( data )

  # Calculate between-cluster sum of squares
  between_ss <- 0
  for ( cluster in unique( cluster_labels ) ) {
    cluster_data <- data[cluster_labels == cluster,]
    cluster_mean <- colMeans( cluster_data )
    between_ss <- between_ss + nrow( cluster_data ) * sum( ( cluster_mean - overall_mean )^2 )
  }

  # Calculate within-cluster sum of squares
  within_ss <- 0
  for ( cluster in unique( cluster_labels ) ) {
    cluster_data <- data[cluster_labels == cluster,]
    cluster_mean <- colMeans( cluster_data )
    within_ss <- within_ss + sum( apply( cluster_data, 1, function( x ) sum( ( x - cluster_mean )^2 ) ) )
  }

  # Calculate Calinski-Harabasz Index
  ch_index <- ( ( n - k ) / ( k - 1 ) ) * ( between_ss / within_ss )
  return( ch_index )
}

# Inertia (Within-Cluster Sum of Squares)
calculate_inertia <- function( data, cluster_labels ) {
  # Ensure data is a matrix
  data <- as.matrix( data )

  # Calculate cluster centers
  unique_labels <- unique( cluster_labels )
  centers <- t( sapply( unique_labels, function( k ) {
    colMeans( data[cluster_labels == k, , drop = FALSE] )
  } ) )

  # Calculate inertia
  total_inertia <- sum( sapply( seq_along( unique_labels ), function( i ) {
    cluster_data <- data[cluster_labels == unique_labels[i], , drop = FALSE]
    sum( apply( cluster_data, 1, function( row ) sum( ( row - centers[i,] )^2 ) ) )
  } ) )

  return( total_inertia )
}


# Normalized Mutual Information
# Helper functions
entropy <- function( labels ) {
  p <- table( labels ) / length( labels )
  -sum( p * log( p ) )
}

calculate_nmi <- function( true_labels, cluster_labels ) {
  contingency <- table( true_labels, cluster_labels )

  # Calculate entropies
  h_true <- entropy( true_labels )
  h_cluster <- entropy( cluster_labels )

  # Calculate mutual information
  n <- sum( contingency )
  mi <- sum( contingency * log( contingency * n / ( rowSums( contingency ) %*% t( colSums( contingency ) ) ) ) )

  # Calculate NMI
  nmi <- 2 * mi / ( h_true + h_cluster )
  return( nmi )
}

# Adjusted Mutual Information
mutual_information <- function( true_labels, cluster_labels ) {
  contingency <- table( true_labels, cluster_labels )
  n <- sum( contingency )
  mi <- 0
  for ( i in seq_len( nrow( contingency ) ) ) {
    for ( j in seq_len( ncol( contingency ) ) ) {
      if ( contingency[i, j] > 0 ) {
        mi <- mi + contingency[i, j] * log( contingency[i, j] * n / ( sum( contingency[i,] ) * sum( contingency[, j] ) ) )
      }
    }
  }
  mi / n
}

expected_mutual_information <- function( true_labels, cluster_labels ) {
  contingency <- table( true_labels, cluster_labels )
  n <- sum( contingency )
  a <- rowSums( contingency )
  b <- colSums( contingency )
  emi <- 0
  for ( i in seq_len( nrow( contingency ) ) ) {
    for ( j in seq_len( ncol( contingency ) ) ) {
      if ( a[i] > 0 && b[j] > 0 ) {
        mij <- max( 1, a[i] + b[j] - n )
        xij <- min( a[i], b[j] )
        for ( nij in mij:xij ) {
          v <- nij / n * log( ( nij * n ) / ( a[i] * b[j] ) )
          emi <- emi + v * exp( lchoose( a[i], nij ) + lchoose( n - a[i], b[j] - nij ) - lchoose( n, b[j] ) )
        }
      }
    }
  }
  emi
}

calculate_ami <- function( true_labels, cluster_labels ) {
  mi <- mutual_information( true_labels, cluster_labels )
  emi <- expected_mutual_information( true_labels, cluster_labels )
  h_true <- entropy( true_labels )
  h_cluster <- entropy( cluster_labels )
  max_h <- max( h_true, h_cluster )

  if ( mi == emi ) {
    return( 0 )
  } else {
    return( ( mi - emi ) / ( max_h - emi ) )
  }
}

# Calculate various cluster indices and return as a data frame
calculate_cluster_scores_df <- function( clustering_results, projection_methods, clustering_methods, cluster_number_methods,
                                         distance_metric = "euclidean", nProc = 2 ) {

  # Validate input
  if ( !distance_metric %in% c( "euclidean", "manhattan", "maximum", "canberra", "binary", "minkowski" ) ) {
    stop( "Unsupported distance metric" )
  }

  cluster_scores <- pbmcapply::pbmclapply( projection_methods, function( projection_method ) {
    if ( is.null( clustering_results[[projection_method]] ) ) {
      warning( "Projection for method ", projection_method, " returned NULL. Skipping." )
      return( NULL )
    }

    cluster_scores_per_alg <- lapply( clustering_methods, function( cluster_alg ) {
      cluster_scores_per_number_method <- lapply( cluster_number_methods, function( cluster_number_method ) {

        combined_result <- clustering_results[[projection_method]][[cluster_alg]][[cluster_number_method]]

        projected_data <- combined_result[, 1:2]
        Target <- combined_result$Target
        Clusters <- combined_result$Cluster

        cluster_accuracy <- sum( Clusters == Target ) / length( Clusters )

        distance_matrix <- stats::dist( projected_data, method = distance_metric )
        Silhouette_index <- mean( cluster::silhouette( Clusters, distance_matrix )[, "sil_width"] )
        Dunn_index <- calculate_dunn_index( distance_matrix = distance_matrix, clusters = Clusters )
        Rand_index <- calculate_adjusted_rand_index( Clusters, Target )
        DaviesBouldin_index <- calculate_davies_bouldin_index( clusters = Clusters, data = projected_data )
        dbcv_index <- calculate_dbcv( data = projected_data, clusters = Clusters, dist_method = distance_metric )
        CalinskiHarabasz_index <- calculate_calinski_harabasz_index( projected_data, Clusters )
        inertia_value <- calculate_inertia( projected_data, Clusters )
        ami_value <- calculate_ami( Target, Clusters )


        return( data.frame(
          projection_method = projection_method,
          clustering_method = cluster_alg,
          cluster_number_method = cluster_number_method,
          cluster_accuracy = cluster_accuracy,
          Silhouette_index = Silhouette_index,
          Dunn_index = Dunn_index,
          Rand_index = Rand_index,
          DaviesBouldin_index = DaviesBouldin_index,
          dbcv_index = dbcv_index,
          CalinskiHarabasz_index = CalinskiHarabasz_index,
          inertia = inertia_value,
          adjusted_mutual_information = ami_value,
          stringsAsFactors = FALSE
        ) )
      } )
      do.call( rbind, cluster_scores_per_number_method )
    } )
    do.call( rbind, cluster_scores_per_alg )
  }, mc.cores = min( length( projection_methods ), nProc ) )

  cluster_scores_df <- do.call( rbind, cluster_scores )
  return( cluster_scores_df )
}


# Rank the metrics and find the best combination of methods
process_cluster_scores <-
  function( cluster_scores_df, selected_cluster_metrics = c( "cluster_accuracy", "Silhouette_index", "Dunn_index",
                                                             "Rand_index", "DaviesBouldin_index", "dbcv_index",
                                                             "CalinskiHarabasz_index", "inertia", "adjusted_mutual_information" ),
            clustering_methods ) {
    # Validate selected metrics
    valid_cluster_metrics <- c( "cluster_accuracy", "Silhouette_index", "Dunn_index", "Rand_index", "DaviesBouldin_index", "dbcv_index",
                                "CalinskiHarabasz_index", "inertia", "adjusted_mutual_information" )
    cluster_metrics_for_orig_classes <- c("Silhouette_index", "DaviesBouldin_index", "CalinskiHarabasz_index" )
    # cluster_metrics_for_orig_classes <- c("CalinskiHarabasz_index" )
    
    if ( length( clustering_methods ) == 1 && clustering_methods == "none" ) {
      selected_cluster_metrics <- intersect( selected_cluster_metrics, cluster_metrics_for_orig_classes )
    } else {
      selected_cluster_metrics <- intersect( selected_cluster_metrics, valid_cluster_metrics )
    }

    if ( length( selected_cluster_metrics ) == 0 ) {
      message( "No valid cluster metrics selected for ranking.\n Reverting to all impleneted metrics: " )
      message( valid_cluster_metrics )
      selected_cluster_metrics <- valid_cluster_metrics
    }

    # Ranking scores: higher is better for most, lower is better for Davies-Bouldin
    rank_columns <- sapply( selected_cluster_metrics, function( metric ) {
      if ( metric %in% c( "DaviesBouldin_index", "inertia" ) ) {
        -cluster_scores_df[[metric]] # Lower is better, so negate values to rank
      } else {
        cluster_scores_df[[metric]] # Higher is better
      }
    } )

    # Helper function for row medians
    rowMedians <- function( x, na.rm = TRUE ) {
      if ( !is.matrix( x ) ) {
        stop( "Input must be a matrix" )
      }

      apply( x, 1, median, na.rm = na.rm )
    }

    # Calculate ranks for each metric
    rank_df <- apply( rank_columns, 2, rank, ties.method = "min" )

    # Create a combined rank by summing across selected metrics
    combined_rank <- rowMedians( rank_df )
    cluster_scores_df$combined_rank <- combined_rank
# 
#     # Find the row with the highest combined rank (best combination)
#     best_index <- which( combined_rank == max( combined_rank ) )
# 
#     # Extract the best combination information
#     best_combination <- as.data.frame( cluster_scores_df[best_index, c( "projection_method", "clustering_method", "cluster_number_method" )] )
# 
#     # Find the row with the second highest combined rank (second best combination)
#     if (length(unique(combined_rank)) > 1) {
#     second_best_index <-     sort(unique(combined_rank), decreasing = TRUE)[2]
#     } else {
#       second_best_index <- best_index
#     }
# 
#     # Extract the second best combination information
#     second_best_combination <- as.data.frame( cluster_scores_df[second_best_index, c( "projection_method", "clustering_method", "cluster_number_method" )] )
    # Add columns for individual ranks to the data frame
    rank_names <- paste0( selected_cluster_metrics, "_rank" )
    cluster_scores_df[rank_names] <- rank_df

    # Return results as a list
    return( 
      ranked_data = cluster_scores_df )
  }

############### Functions for plotting ###############

# Function to plot the projected data and prior classes or clusters
create_dataset_plots <- function( clustering_results, projection_methods, clustering_methods, cluster_number_methods,
                                  dataset_name, label_points, highlight_misclassified, points_colored_for, cells_colored_for,
                                  palette_target = NULL, palette_cluster = NULL, nProc = 2 ) {

  projection_plots <- pbmcapply::pbmclapply( projection_methods, function( projection_method ) {

    cluster_algs_plots <- lapply( clustering_methods, function( cluster_alg ) {
      cluster_number_method_plots <- lapply( cluster_number_methods, function( cluster_number_method ) {

        combined_result <- clustering_results[[projection_method]][[cluster_alg]][[cluster_number_method]]
        
        if ( is.null( combined_result )  ) {
          dfPlot <- data.frame( x = 1, y = 1 )
          text_to_show <- "Projection\nfailed"
          plot <- ggplot( data = dfPlot, aes( x = x, y = y, color = x, fill = y ) ) +
            geom_text( label = text_to_show, show.legend = FALSE ) +
            theme_light( ) +
            labs( x = "Dim 1", y = "Dim 2" )
        } else {

          # Initialize variables
          max_attempts <- 10
          plot <- NULL
          
          # Loop for attempting to plot with increasing jitter
          for (attempt in 1:max_attempts) {
            plot1 <- tryCatch({
              plotVoronoiTargetProjection(
                X = combined_result[, 1:2],
                targets = combined_result$Target,
                clusters = combined_result$Cluster,
                labels = combined_result$Label,
                misclassified = combined_result$Misclassified,
                LabelPoints = label_points,
                ColorMisClassified = highlight_misclassified,
                points_colored_for = points_colored_for,
                cells_colored_for = cells_colored_for,
                palette_target = palette_target,
                palette_cluster = palette_cluster,
                cluster_alg = cluster_alg
              )
            }, error = function(e) {
              return(structure(list(message = e$message), class = "try-error"))
            })
            
            # Check if the plot was successful
            if ((!inherits(plot1, "try-error") && !is.null(plot1)) || inherits(plot1, "gg")) {
              plot <- plot1
              break  # Exit the loop if successful
            } else {
              # Prepare columns for jittering
              columns_to_jitter <- intersect(setdiff(names(combined_result),
                                                     c("Cluster", "Target", "Label", "Misclassified")),
                                             names(combined_result))
              numeric_columns_to_jitter <- sapply(combined_result[, columns_to_jitter], is.numeric)
              
              # Apply jitter with increasing factor based on the attempt number
              jitter_factor <- attempt * 10  # Increase jitter factor with each attempt
              combined_result[, columns_to_jitter[numeric_columns_to_jitter]] <-
                lapply(combined_result[, columns_to_jitter[numeric_columns_to_jitter], drop = FALSE], function(x) jitter(x, factor = jitter_factor))
            }
          }
          
          # If all attempts failed, create a fallback plot
          if (is.null(plot)) {
            dfPlot <- data.frame(x = 1, y = 1)
            text_to_show <- "Projection\nfailed"
            plot <- ggplot(data = dfPlot, aes(x = x, y = y, color = x, fill = y)) +
              geom_text(label = text_to_show, show.legend = FALSE) +
              theme_light() +
              labs(x = "Dim 1", y = "Dim 2")
          }
        }
        
        plot_title <- paste( dataset_name, ": ", projection_method )
        if ( cluster_alg != "none" ) {
          plot_title <- paste( plot_title, "- ", cluster_number_method, "- ", cluster_alg )
        }
        plot <- plot + labs( title = plot_title )
        if ( cluster_alg == "none" )
          plot <- plot + labs( fill = "Target" )

        return( plot ) # Ensuring plot is returned
      } )
      names( cluster_number_method_plots ) <- cluster_number_methods
      return( cluster_number_method_plots )
    } )
    names( cluster_algs_plots ) <- clustering_methods
    return( cluster_algs_plots )
  }, mc.cores = min( length( projection_methods ), nProc ) )
  names( projection_plots ) <- projection_methods
  return( projection_plots )
}


# Function to plot Voronoi cells with projection and clustering results
plotVoronoiTargetProjection <- function( X, targets, clusters = NULL,
                                         labels = NULL, LabelPoints = FALSE,
                                         misclassified = NULL, ColorMisClassified = FALSE,
                                         points_colored_for = "Target", cells_colored_for = "Cluster",
                                         palette_target = NULL, palette_cluster = NULL, 
                                         cluster_alg = NULL) {
  # Extended colorblind palette
  cb_palette <- c(
    "#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00",
    "#CC79A7", "#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"
  )
  shape_values <- c( 17, 16, 15, 18, 2, 1, 0, 5, 7, 8, 9, 19, 11, 12 )

  # Initial checks of arguments
  if ( ncol( X ) < 2 ) stop( "The input data `X` must have at least two columns." )
  if ( nrow( X ) != length( targets ) ) stop( "The length of `targets` must match the number of rows in `X`." )

  if ( !is.null( clusters ) && length( clusters ) != nrow( X ) )
    stop( "The length of `clusters` must match the number of rows in `X`." )
  if ( !is.null( misclassified ) && length( misclassified ) != length( labels ) )
    stop( "The length of `misclassified` must match the number of labels." )

  if ( !is.null( palette_target ) && length( palette_target ) < length( unique( targets ) ) ) {
    stop( "The `palette_target` must provide enough color values for the number of target groups." )
  }
  if ( !is.null( palette_cluster ) && length( palette_cluster ) < length( unique( clusters ) ) ) {
    stop( "The `palette_cluster` must provide enough color values for the number of clusters." )
  }

  # Define default palettes if none are provided
  if ( is.null( palette_target ) ) palette_target <- rep( cb_palette, length.out = length( unique( targets ) ) )
  if ( is.null( palette_cluster ) ) palette_cluster <- rep( cb_palette, length.out = length( unique( clusters ) ) )

  # Assign labels if not present
  if ( is.null( labels ) ) {
    if ( !is.null( rownames( X ) ) ) {
      labels <- rownames( X )
    } else {
      labels <- seq_len( nrow( X ) )
    }
  }

  # Default values for clusters and misclassified if NULL
  if ( is.null( clusters ) ) clusters <- targets
  if ( is.null( misclassified ) ) misclassified <- rep( 0, nrow( X ) )

  # Data preparation
  plotData <- data.frame( Proj1 = X[, 1], Proj2 = X[, 2], Target = targets,
                          Clusters = clusters, Label = labels, Misclassified = misclassified )

  plotData$DotsInformation <- if ( points_colored_for == "Target" ) plotData$Target else plotData$Clusters
  plotData$CellsInformation <- if ( cells_colored_for == "Cluster" ) plotData$Clusters else plotData$Target

  # Voronoi diagram computation
  voronoiData <- deldir::deldir( plotData$Proj1, plotData$Proj2 )

  # Convert Voronoi tessellation to a data frame for plotting
  vor_polys <- deldir::tile.list( voronoiData )
  voronoi_df <- do.call( rbind, lapply( seq_along( vor_polys ), function( i ) {
    data.frame( x = vor_polys[[i]]$x, y = vor_polys[[i]]$y, id = i )
  } ) )

  # Create plot with ggplot2
  plot <- ggplot2::ggplot( ) +
    ggplot2::geom_polygon( data = voronoi_df, ggplot2::aes( x = x, y = y, group = id, fill = as.factor( plotData$CellsInformation[id] ) ),
                           alpha = 0.3, color = NA ) +
    ggplot2::geom_point( data = plotData, ggplot2::aes( x = Proj1, y = Proj2,
                                                        color = as.factor( DotsInformation ), shape = as.factor( DotsInformation ) ) ) +
    ggplot2::theme_light( ) +
    ggplot2::theme( legend.position = "inside", legend.position.inside = c( 0.5, 0.08 ), legend.direction = "horizontal",
                    legend.box = "horizontal",
                    legend.background = element_rect( color = "transparent", fill = ggplot2::alpha( "white", 0.2 ) ) ) +
    #ggplot2::labs( x = "Dim 1", y = "Dim 2", color = "Target", fill = "Cluster", shape = "Target" ) +
    scale_shape_manual( values = shape_values ) +
    ggplot2::scale_fill_manual( values = palette_cluster ) +
    ggplot2::scale_color_manual( values = palette_target ) +
    guides( shape = guide_legend( override.aes = list( label = "" ) ) )
  
  if (cluster_alg == "none") {
    plot <- plot + ggplot2::labs( x = "Dim 1", y = "Dim 2", color = "Target", fill = "Target", shape = "Target" ) 
} else {
  plot <- plot + ggplot2::labs( x = "Dim 1", y = "Dim 2", color = "Target", fill = "Cluster", shape = "Target" ) 
}
  # Conditional labeling of points
  if ( LabelPoints ) {
    plot <- plot +
      ggrepel::geom_text_repel( data = plotData,
                                ggplot2::aes( x = Proj1, y = Proj2, color = as.factor( DotsInformation ),
                                              label = Label, fontface = 2 ), vjust = -1, size = 3, max.overlaps = Inf )
  }

  # Color misclassified cases in red
  if ( sum( plotData$Misclassified ) > 0 & ColorMisClassified ) {
    suppressMessages( {
      plot <- plot +
        geom_text(
          aes( x = Inf, y = Inf,
               label = paste0( round( 100 * sum( plotData$Misclassified ) / nrow( X ), 1 ), "% misclassified" ) ),
          hjust = 1,
          vjust = 1
        )
    } )
    q <- ggplot_build( plot )
    q$data[[2]]$colour[plotData$Misclassified == 1] <- "red"
    gtable <- ggplot2::ggplot_gtable( q )
    plot <- ggplotify::as.ggplot( function( ) grid::grid.draw( gtable ) )
  }

  return( plot )
}


# Function to extract all plots from the generated lists and combine them into a figure
combine_all_plots <- function( datasets, projection_plots, projection_methods, clustering_methods, cluster_number_methods , nProc = 2) {
  if ( any( sapply( list( datasets, projection_plots, projection_methods, clustering_methods, cluster_number_methods ), is.null ) ) ) {
    stop( "All input lists must be non-null and contain elements." )
  }

  if ( any( sapply( list( datasets, projection_plots, projection_methods, clustering_methods, cluster_number_methods ), length ) == 0 ) ) {
    stop( "All input lists must contain non-empty elements." )
  }

  calculate_plots_per_dataset <- function( )
    length( projection_methods ) *
      length( clustering_methods ) *
      length( cluster_number_methods )

  plots_per_dataset <- calculate_plots_per_dataset( )

  if ( plots_per_dataset == 0 ) {
    stop( "No valid combinations found in input lists." )
  }

  all_plots <- unlist( lapply( projection_plots, function( projection ) {
    if ( !is.null( projection ) ) {
      return( unlist( projection, recursive = FALSE, use.names = FALSE ) )
    }
  } ), recursive = FALSE, use.names = FALSE )

  if ( length( all_plots ) == 0 ) {
    stop( "No plots available to combine." )
  }

  figure_count <- length( all_plots ) %/% plots_per_dataset

  if ( figure_count == 0 ) {
    stop( "Insufficient plots to create figures based on input methods." )
  }

  if ( length( clustering_methods ) == 1 && clustering_methods == "none" ) {
    columns <- calculate_golden_matrix_dims( n_items = length( projection_methods ) )$ncol
    rows <- calculate_golden_matrix_dims( n_items = length( projection_methods ) )$nrow
  } else {
    columns <- length( clustering_methods ) * length( cluster_number_methods )
    rows <- length( projection_methods )
  }

  create_combined_plot <- function( idx ) {
    start_idx <- 1 + ( idx - 1 ) * plots_per_dataset
    end_idx <- plots_per_dataset + ( idx - 1 ) * plots_per_dataset

    if ( end_idx > length( all_plots ) ) {
      stop( sprintf( "Index out of range when creating plot: %s", idx ) )
    }

    if ( length( clustering_methods ) == 1 && clustering_methods == "none" ) {
      fig <- cowplot::plot_grid( plotlist = all_plots[start_idx:end_idx], ncol = columns, nrow = rows, labels = "AUTO" )
    } else {
      fig <- cowplot::plot_grid( plotlist = all_plots[start_idx:end_idx], ncol = columns, nrow = rows,
                                 labels = paste0( rep( LETTERS[1:nrow], each = ncol ), rep( 1:ncol, nrow ) ) )
    }
    return( fig )
  }

  if ( figure_count > 1 ) {
    figures <- pbmcapply::pbmclapply(
      seq_along( datasets ),
      create_combined_plot,
      mc.cores = min( figure_count, nProc )
    )
  } else {
    figures <- lapply( seq_along( datasets ), create_combined_plot )
  }

  names( figures ) <- datasets
  return( figures )
}


# Function to calculate a compact matrix for a combined results figure 
calculate_compact_matrix_dims <- function( n_items ) {
  if ( n_items < 1 ) {
    stop( "Number of items must be at least 1." )
  }

  # Calculate the number of rows (ceiling of square root)
  nrow <- ceiling( sqrt( n_items ) )

  # Calculate the number of columns (ceiling of items divided by rows)
  ncol <- ceiling( n_items / nrow )

  return( list( nrow = nrow, ncol = ncol ) )
}


calculate_full_matrix_dims <- function( n_items, vertical = TRUE ) {
  # Find all possible factors of n
  factors <- which( n_items %% 1:n_items == 0 )

  # Find the pair of factors that minimizes the difference between rows and columns
  best_pair <- which.min( abs( factors - n_items / factors ) )


  # Determine nrow and ncol
  if ( vertical ) {
    ncol <- min( factors[best_pair], n_items / factors[best_pair] )
    nrow <- max( factors[best_pair], n_items / factors[best_pair] )
  } else {
    nrow <- min( factors[best_pair], n_items / factors[best_pair] )
    ncol <- max( factors[best_pair], n_items / factors[best_pair] )
  }
  # Return the result as a named list
  return( list( nrow = nrow, ncol = ncol ) )
}

calculate_golden_matrix_dims <- function( n_items, vertical = TRUE, golden_ratio = 1.618, max_ratio = 4 ) {
  if ( n_items <= 0 ) {
    stop( "n_items must be a positive integer" )
  }

  # Find all possible factors of n_items
  factors <- which( n_items %% 1:n_items == 0 )

  # Calculate potential ratios for each pair of factors
  ratios <- factors / ( n_items / factors )
  inverted_ratios <- ( n_items / factors ) / factors

  # Filter out invalid ratios
  valid_indices <- which( ratios <= max_ratio & inverted_ratios <= max_ratio )

  if ( length( valid_indices ) == 0 || max( factors[valid_indices] ) == n_items ) {
    # Revert to compact matrix calculation if no valid indices or single row
    nrow <- ceiling( sqrt( n_items ) )
    ncol <- ceiling( n_items / nrow )

    if ( !vertical ) {
      temp <- nrow
      nrow <- ncol
      ncol <- temp
    }
  } else {
    # Find the pair of factors closest to the golden ratio
    best_index <- valid_indices[which.min( abs( ratios[valid_indices] - golden_ratio ) )]
    best_factor <- factors[best_index]

    if ( vertical ) {
      ncol <- min( best_factor, n_items / best_factor )
      nrow <- max( best_factor, n_items / best_factor )
    } else {
      nrow <- min( best_factor, n_items / best_factor )
      ncol <- max( best_factor, n_items / best_factor )
    }
  }

  # Calculate how full the matrix is
  fullness <- n_items / ( nrow * ncol )

  # Return the result as a named list
  return( list( nrow = nrow, ncol = ncol, fullness = fullness, ratio = max( nrow / ncol, ncol / nrow ) ) )
}